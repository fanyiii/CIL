\documentclass[10pt,conference,compsocconf]{IEEEtran}

%\usepackage{times}
%\usepackage{balance}
\usepackage{url}
\usepackage{graphicx}	% For figure environment
\usepackage{tikz}
\usepackage{pgfplots}
\usetikzlibrary{calc,intersections,through,backgrounds, shapes}
\newcommand{\rfig}[1]{Figure~\ref{fig:#1}}


\begin{document}
\title{CIL Project: Collaborative Filtering}

\author{
	Ulla Aeschbacher, David Eschbach,  Xiyue Shi, Fanyi Zhou\\
	Department of Computer Science, ETH Zurich, Switzerland
}

\maketitle

\begin{abstract}
Recommender systems and collaborative filtering have become very popular lately. For example streaming platforms use them to recommend new movies to their users based on ratings that users have previously assigned to other movies. For this purpose recommender systems try to exploit similarities between the rated items and potentially also between users. To achieve this, collaborative filtering aims to learn a low dimensional representation of those items.

In this paper we present our approach to the problem, which is based on a combination of Singular Value Decomposition (SVD) and k-nearest-neighbors (KNN) working with the Pearson correlation. Our final solution has a Root Mean Square Error (RMSE) of 0.98362.
\end{abstract}

\section{Introduction}
Collaborative filtering has been known for a while and there are many important contributions by research. For example in \cite{breese} there is an overview and an empirical analysis of multiple algorithms including memory-based algorithms predicting the rating of an item by the active user based on a weighted sum of the other users' rating of that item. Another example taken from \cite{breese} are model-based algorithms like the Bayesian Network Model that take a probabilistic approach to the problem by setting the rating to its expected value according to some probability distribution. In \cite{paterek} the authors focus on SVD and in particular elaborate on the importance of regularization techniques. They further demonstrate that the combination of several distinct predictors can lead to even better results. Many of the more recent contributions make use of deep neural networks. For example in \cite{he} the authors introduce neural collaborative filtering. This is a neural network technique based on a combination of generalized matrix factorization and multi-layer perceptron optimizing the logarithm of a likelihood. In \cite{sedhain} the authors rely on autoencoders to solve the problem of collaborative filtering. One of the main advantages of their system AutoRec compared to multiple other collaborative filtering solutions is that it directly optimizes the Root Mean Square Error (RMSE). Other contributions rely on clustering techniques like k-nearest neighbors. For example \cite{xue} presents a cluster-based collaborative filtering framework that puts much effort into smoothing clusters.

Similar to \cite{paterek}, our contribution is based on a combination of multiple predictors. This predictors include KNN based solutions and dimensionality reduction through SVD. We also experimented with neural networks in particular by building a bidirectional Long Short-Term Memory (LSTM) network that directly predicts ratings. Furthermore we tried to achieve the dimensionality reduction with an autoencoder built of LSTM cells. However, both of these aproaches did not yield an improvement on our results, so we decided to drop them.

\section{Our Novel Solution}
Our novel solution is based on the idea of fusion by ensemble averaging, a process of creating multiple models and averaging their outputs to produce a desired output\cite{ariel}. Usuallly a model created by ensemble averaging performs better than any individual model, because it "averages out" errors of each individual model. To evaluate our models, we use a random 80:20 splits over the original training dataset. All algorithms ran on the same splits. 

For this project, we first constructed k-NN models with different similarity metrics and different perspectives, and SVD++ models with different sizes of factors (the same values were used for other parameters). In Table I, you can see the Root-Mean-Squared-Error of each modle. Item based k-NN models perform better than user based k-NN models, and k=NN models based on Pearson correlation similarity perform better than k-NN models based on Mean Squared Difference (MSD) similarity. There is no much difference between SVD++ models with different sizes of factors. Among those individual models, item-item k-NN model based on Pearson correlation similarity performs best, and its RMSE on our validation set is 0.991212.

\begin{table}[h] 
\begin{tabular}{|l|l|}
\hline
\textbf{Model}                                                   & \textbf{RMSE} \\ \hline
User-User k-NN model based on MSD similarity                     & 1.010925     \\ \hline
Item-Item k-NN model based on MSD similarity                     & 0.995846     \\ \hline
User-User k-NN model based on Pearson correlation similarity     & 0.999997     \\ \hline
Item-Item k-NN model based on Pearson correlation similarity     & 0.991212     \\ \hline
SVD++ model factors=10                                           & 1.003788     \\ \hline
SVD++ model factors=20                                           & 1.003131     \\ \hline
SVD++ model factors=30                                           & 1.003923     \\ \hline
\end{tabular}
\caption{\label{tab:Table 1}RMSE of different models}
\end{table}

A simple fusion of these models can then be contructed by averaging outputs of those basic models. We tried to construct fusion models with different combinations, and you can see results of part of our experiments in Table II. All fusion models perform significantly better than individual models. The model which averages outputs of SVD++ model with factor size equals to 20, User-User k-NN model based on Pearson correlation similarity and Item-Item k-NN model based on Pearson correlation similarity achieved the best result, and its RMSE on our validation set is 0.987597. Compared the model with the best individual model in our previous experiments, the fusion model reduces the RMSE by 3.647\%. With the model, we achieved a score of 0.98362 on kaggle, which is the best among all of our submissions.

\begin{table}[h]
\begin{tabular}{|l|l|}
\hline
\textbf{Fusion Model}                                          & \textbf{RMSE} \\ \hline
k-NN-User-User-Pearson + k-NN-Item-Item-Pearson                & 0.990154     \\ \hline
k-NN-Item-Item-MSD + k-NN-Item-Item-Pearson                    & 0.988693     \\ \hline
k-NN-User-User-MSD + k-NN-User-User-Pearson                    & 0.990045     \\
+ k-NN-Item-Item-MSD + k-NN-Item-Item-Pearson                  &      \\ \hline
SVD++ factors=10 + SVD++ factors=20         & 0.999779     \\ 
+ SVD++ factors=30 & \\\hline
SVD++ factors=20 + k-NN-User-User-Pearson                      & 0.987597     \\
+ k-NN-Item-Item-Pearson                                       &      \\ \hline
SVD++ factors=20 + k-NN-Item-Item-MSD                          & 0.988114     \\
+ k-NN-Item-Item-Pearson                                       &  \\ \hline
Fusion of all models                                           & 0.989956     \\ \hline
\end{tabular}
\caption{\label{tab:Table 2}RMSE of fusion models}
\end{table}

\section{Comparing To Baselines}
We are comparing our novel solution to two others such that we can see how much improvement was achieved. The first of the two other solutions is the one from Assignment 3. There we were first completing the missing ratings in the matrix by setting them to the average over all observed ratings for a particular item. Then we tried to find an underlying structure in the data by performing SVD, where $X^{train} = UDV^T$. We varied the number $k$ of eigenvalues used and truncated $U$ and $V$ accordingly, by taking the first $k$ columns of each of the matrixes. \rfig{rmse} shows the Root-Mean-Squared-Error for $k$ between 0 and 95, in intervals of 5. In \rfig{rmse_detail} we can see the error more clearly for $k$ between 5 and 95. Apparently, taking the biggest 10 eigenvalues is giving us the best approximation to the data structure. The best result we achieved on kaggle this way was an error of 1.16473.

\begin{figure}[bp]
\centering
\begin{tikzpicture}
	\begin{axis}[
	xlabel={$k$},
	ylabel={\small{RMSE}},
	xmin=-5, xmax=100,
	ymin=0, ymax=4.5,
	xtick={5,15,25,35,45,55,65,75,85,95},
	ytick={1,2,3,4},
	x tick label style={color=black},
	legend style={
		draw=black,
		fill=white, 
		legend pos= north east,
		cells={anchor=west},},
	]
	\addplot[
		color=blue,
		mark=o,
		thick] 
		coordinates {(0,4.016120664688759)(5,1.0825649908614905)( 10,1.0815540472050764)(15,1.082655476468529)(20,1.084039420552301)(25, 1.08544809077967)(30,1.0870430839468697)(35,1.0883525165436614)(40,1.0894315213881094)(45,1.0906340685662301)(50,1.092158368755972)(55,1.0934254934023788)(60,1.0946167799674307)(65,1.0957557542153566)(70,1.096642058025502)(75,1.0975006311104656)(80,1.0980612147506275)(85,1.0989472280119372)(90,1.099854525492459)(95,1.1005504083912052)};
	\end{axis}
\end{tikzpicture}
\caption{RMSE in SVD with different $k$}
\label{fig:rmse}
\end{figure}

\begin{figure}[tbp]
\centering
\begin{tikzpicture}
	\begin{axis}[
	xlabel={$k$},
	ylabel={\small{RMSE}},
	xmin=0, xmax=100,
	ymin=1.08, ymax=1.104,
	xtick={5,15,25,35,45,55,65,75,85,95},
	ytick={1.08,1.082,1.084,1.086,1.088,1.090,1.092,1.094,1.096,1.098,1.1,1.102},
	yticklabel style={/pgf/number format/.cd,fixed,precision=3},
	legend style={
		draw=black,
		fill=white, 
		legend pos= north east,
		cells={anchor=west},},
	]
	\addplot[
		color=blue,
		mark=o,
		thick] 
		coordinates {(5,1.0825649908614905)( 10,1.0815540472050764)(15,1.082655476468529)(20,1.084039420552301)(25, 1.08544809077967)(30,1.0870430839468697)(35,1.0883525165436614)(40,1.0894315213881094)(45,1.0906340685662301)(50,1.092158368755972)(55,1.0934254934023788)(60,1.0946167799674307)(65,1.0957557542153566)(70,1.096642058025502)(75,1.0975006311104656)(80,1.0980612147506275)(85,1.0989472280119372)(90,1.099854525492459)(95,1.1005504083912052)};
	\end{axis}
\end{tikzpicture}
\caption{RMSE in SVD with different $k$, shown in more detail}
\label{fig:rmse_detail}
\end{figure}

The second solution we are presenting here for comparison is a neural network. We trained a bidirectional LSTM. It has a bias layer, so that we could take into account that different people are rating higher or lower in general. We tried different dropout values, numbers of nodes, batch-sizes and trained over various epochs. In \rfig{lstm} shows the Mean-Squared-Error loss over ten epochs of the neural network. We set a dropout of 0.75, 40 nodes in each layer and a batch size of 50, as these values were performing the best in our experiment. The best result we achieved on kaggle this way was an error of 1.04627. There are people that achieved much better results with a neural network \cite{tseng}, but our feeling is that our data was too sparse for this.

\begin{figure}[tbp]
\centering
\begin{tikzpicture}
	\begin{axis}[
	xlabel={Epochs},
	ylabel={\small{MSE}},
	xmin=0, xmax=11,
	ymin=1.00, ymax=1.35,
	xtick={1,2,3,4,5,6,7,8,9,10},
	ytick={1.0,1.1,1.2,1.3},
	yticklabel style={/pgf/number format/.cd,fixed,precision=3},
	legend style={
		draw=black,
		fill=white, 
		legend pos= north east,
		cells={anchor=west},},
	]
	\addplot[
		color=blue,
		mark=o,
		thick] 
		coordinates {(1,1.309)(2,1.032)(3,1.022)(4,1.017)(5,1.015)(6,1.014)(7,1.013)(8,1.013)(9,1.011)(10,1.010)};
	\end{axis}
\end{tikzpicture}
\caption{MSE loss in LSTM over 10 epochs.}
\label{fig:lstm}
\end{figure}

Comparing the two solutions with our novel approach result of 0.98362 we can see that we managed to get much better results with.

\section{Summary}
In summary, our novel solution adopts an ensemble learning approach and averages the results of different prediction algorithms, namely SVD, KNN-User-User-Pearson and KNN-Item-Item-Pearson. Traditional ensemble learning approaches usually train multiple models of a single learning algorithm. Here, our novel fusion approach managed to generate better results by first choosing the best parameters for the base collaborative filtering algorithms, then combining models with different algorithms as well as different parameters of the same algorithm as in a traditional approach. Experimental results suggest consistent better performance when several different algorithms are used. The final ensemble is then selected based on  cross-validation results in a systematic comparison of different algorithm combinations. 
This approach is also flexible and time-efficient in the case where more base CF algorithms are added and cross-validation needs to be carried out for a new ensemble. 
Since our solution computes a simple average of selected models, investigation into the performance of a weighted sum and how to obtain the optimal weights in the final ensemble might be required to achieve further improvement to the prediction results.

\nocite{*}
\bibliographystyle{IEEEtran}
\bibliography{report}
\end{document}
